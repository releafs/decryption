name: Image Processing Pipeline

on:
  push:
    branches:
      - main
    paths:
      - 'decryption/input/**'  # Trigger when a file is pushed to 'decryption/input/'

jobs:
  process-images:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        chunk_id: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  # Divide the task into 10 chunks, adjust this based on file size

    permissions:
      contents: write  # Allow pushing changes to the repository

    steps:
    # Step 1: Check out the repository
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        persist-credentials: true  # Ensure Git credentials are available

    # Step 2: Set up caching for Python dependencies
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    # Step 3: Set up Python environment
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'

    # Step 4: Install dependencies (only if not cached)
    - name: Install Dependencies
      run: |
        pip install --no-cache-dir -r requirements.txt

    # Step 5: Create input directory if missing
    - name: Create input directory if missing
      run: mkdir -p decryption/input

    # Step 6: Split files into chunks for parallel processing
    - name: Split files for parallel processing
      run: |
        total_files=$(ls decryption/input | wc -l)
        files_per_chunk=$((total_files / 10))
        start=$((${{ matrix.chunk_id }} * files_per_chunk))
        end=$((start + files_per_chunk))
        ls decryption/input | sed -n "${start},${end}p" > decryption/chunk_files.txt
        echo "Processing chunk ${{ matrix.chunk_id }}: $(cat decryption/chunk_files.txt)"

    # Step 7: Process each chunk of files using script1.py, script2.py, script3.py
    - name: Process chunk with script1.py
      run: |
        cat decryption/chunk_files.txt | xargs -I {} python scripts/script1.py decryption/input/{}

    - name: Process chunk with script2.py
      run: |
        cat decryption/chunk_files.txt | xargs -I {} python scripts/script2.py decryption/input/{}

    - name: Process chunk with script3.py
      run: |
        cat decryption/chunk_files.txt | xargs -I {} python scripts/script3.py decryption/input/{}

    # Step 8: Ensure the process directory exists and list files
    - name: List processed files
      run: ls -la process/

    # Step 9: Verify if the output file exists
    - name: Verify output file exists
      run: |
        if [ ! -f "process/merged_data_with_metadata.csv" ]; then
          echo "Output file not found!"
          exit 1
        else
          echo "Output file found!"
        fi

    # Step 10: Commit and push processed result to the repository
    - name: Commit and push processed result
      run: |
        git config --global user.email "action@github.com"
        git config --global user.name "GitHub Action"
        git add process/merged_data_with_metadata.csv
        git commit -m "Add processed result for chunk ${{ matrix.chunk_id }}"
        git push

    # Step 11: Clean up processed files after commit
    - name: Clean up process directory
      run: rm -rf process/*
